### File Structure
```
â””â”€â”€ ğŸ“LRS
    â””â”€â”€ ğŸ“video
        â””â”€â”€ .gitignore
        â””â”€â”€ ğŸ“config
            â””â”€â”€ lrs2.yaml
            â””â”€â”€ lrs3.yaml
        â””â”€â”€ ğŸ“cross-modal-sync
        â””â”€â”€ ğŸ“datamodule
            â””â”€â”€ av_dataset.py
            â””â”€â”€ data_module.py
            â””â”€â”€ transforms.py
            â””â”€â”€ turbojpeg.py
            â””â”€â”€ video_length.npy
        â””â”€â”€ ğŸ“espnet
            â””â”€â”€ ğŸ“asr
                â””â”€â”€ asr_utils.py
            â””â”€â”€ ğŸ“nets
                â””â”€â”€ batch_beam_search.py
                â””â”€â”€ beam_search.py
                â””â”€â”€ ctc_prefix_score.py
                â””â”€â”€ e2e_asr_common.py
                â””â”€â”€ lm_interface.py
                â””â”€â”€ ğŸ“pytorch_backend
                    â””â”€â”€ ğŸ“backbones
                        â””â”€â”€ conv1d_extractor.py
                        â””â”€â”€ conv3d_extractor.py
                        â””â”€â”€ ğŸ“modules
                                â””â”€â”€ resnet.cpython-311.pyc
                                â””â”€â”€ resnet.cpython-39.pyc
                                â””â”€â”€ resnet1d.cpython-311.pyc
                                â””â”€â”€ resnet1d.cpython-39.pyc
                            â””â”€â”€ resnet.py
                            â””â”€â”€ resnet1d.py
                            â””â”€â”€ shufflenetv2.py
                    â””â”€â”€ ctc.py
                    â””â”€â”€ e2e_asr_transformer.py
                    â””â”€â”€ ğŸ“lm
                        â””â”€â”€ __init__.py
                        â””â”€â”€ default.py
                        â””â”€â”€ seq_rnn.py
                        â””â”€â”€ transformer.py
                    â””â”€â”€ nets_utils.py
                    â””â”€â”€ ğŸ“transformer
                        â””â”€â”€ __init__.py
                        â””â”€â”€ add_sos_eos.py
                        â””â”€â”€ attention.py
                        â””â”€â”€ convolution.py
                        â””â”€â”€ decoder_layer.py
                        â””â”€â”€ decoder.py
                        â””â”€â”€ embedding.py
                        â””â”€â”€ encoder_layer.py
                        â””â”€â”€ encoder.py
                        â””â”€â”€ label_smoothing_loss.py
                        â””â”€â”€ layer_norm.py
                        â””â”€â”€ mask.py
                        â””â”€â”€ multi_layer_conv.py
                        â””â”€â”€ optimizer.py
                        â””â”€â”€ plot.py
                        â””â”€â”€ positionwise_feed_forward.py
                        â””â”€â”€ raw_embeddings.py
                        â””â”€â”€ repeat.py
                        â””â”€â”€ subsampling.py
                â””â”€â”€ scorer_interface.py
                â””â”€â”€ ğŸ“scorers
                    â””â”€â”€ __init__.py
                    â””â”€â”€ ctc.py
                    â””â”€â”€ length_bonus.py
            â””â”€â”€ ğŸ“utils
                â””â”€â”€ cli_utils.py
                â””â”€â”€ dynamic_import.py
                â””â”€â”€ fill_missing_args.py
        â””â”€â”€ ğŸ“ko_aihub
            â””â”€â”€ ğŸ“test
                â””â”€â”€ ğŸ“lip_J_1_F_03_C282_A_001
                    â””â”€â”€ lip_J_1_F_03_C282_A_001_11.pkl
                        ...
                â””â”€â”€ ğŸ“lip_J_1_F_03_C282_A_002
                    ...
            â””â”€â”€ ğŸ“train
            â””â”€â”€ ğŸ“val

        â””â”€â”€ ğŸ“LRS2
        â””â”€â”€ ğŸ“LRS2_YOLO (...output of prepare code)
                â””â”€â”€ ğŸ“main
                    â””â”€â”€ ğŸ“5535415699068794046
                        â””â”€â”€ 00001.mp4
                        â””â”€â”€ 00001.txt
                        â””â”€â”€ 00002.mp4
                        â””â”€â”€ 00002.txt
                        â””â”€â”€ 00003.mp4
                        â””â”€â”€ 00003.txt
                        â””â”€â”€ ...
                    â””â”€â”€ ğŸ“pretrain
                        â””â”€â”€ ...

                â””â”€â”€ split_data.py
            â””â”€â”€ prepare_LRS2.py
            â””â”€â”€ prepare_LRS3.py
            â””â”€â”€ prepare_Vox2.py
            â””â”€â”€ transcribe_whisper.py
            â””â”€â”€ utils.py
            â””â”€â”€ yolov8n-face.pt
        â””â”€â”€ README.md
        â””â”€â”€ requirements0114_train_success.txt
        â””â”€â”€ setup.sh
        â””â”€â”€ ğŸ“spm
            â””â”€â”€ spm_encode.py
            â””â”€â”€ spm_train.py
            â””â”€â”€ train.sh
            â””â”€â”€ ğŸ“unigram
                â””â”€â”€ unigram5000_units.txt
                â””â”€â”€ unigram5000.model
        â””â”€â”€ utils.py
        â””â”€â”€ Vox+LRS2+LRS3.ckpt
        â””â”€â”€ vq-wav2vec_kmeans.pt
```
---
### Acknowledgement of Original Authors

```
@inproceedings{ott2019fairseq,
  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},
  year = {2019},
}

@inproceedings{ma2023auto,
  author={Ma, Pingchuan and Haliassos, Alexandros and Fernandez-Lopez, Adriana and Chen, Honglie and Petridis, Stavros and Pantic, Maja},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title={Auto-AVSR: Audio-Visual Speech Recognition with Automatic Labels}, 
  year={2023},
}

@inproceedings{watanabe2018espnet,
  author={Shinji Watanabe and Takaaki Hori and Shigeki Karita and Tomoki Hayashi and Jiro Nishitoba and Yuya Unno and Nelson {Enrique Yalta Soplin} and Jahn Heymann and Matthew Wiesner and Nanxin Chen and Adithya Renduchintala and Tsubasa Ochiai},
  title={{ESPnet}: End-to-End Speech Processing Toolkit},
  year={2018},
  booktitle={Proceedings of Interspeech},
  pages={2207--2211},
  doi={10.21437/Interspeech.2018-1456},
  url={http://dx.doi.org/10.21437/Interspeech.2018-1456}
}
@inproceedings{hayashi2020espnet,
  title={{Espnet-TTS}: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit},
  author={Hayashi, Tomoki and Yamamoto, Ryuichi and Inoue, Katsuki and Yoshimura, Takenori and Watanabe, Shinji and Toda, Tomoki and Takeda, Kazuya and Zhang, Yu and Tan, Xu},
  booktitle={Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7654--7658},
  year={2020},
  organization={IEEE}
}
@inproceedings{inaguma-etal-2020-espnet,
    title = "{ESP}net-{ST}: All-in-One Speech Translation Toolkit",
    author = "Inaguma, Hirofumi  and
      Kiyono, Shun  and
      Duh, Kevin  and
      Karita, Shigeki  and
      Yalta, Nelson  and
      Hayashi, Tomoki  and
      Watanabe, Shinji",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-demos.34",
    pages = "302--311",
}
@article{hayashi2021espnet2,
  title={{ESP}net2-{TTS}: Extending the edge of {TTS} research},
  author={Hayashi, Tomoki and Yamamoto, Ryuichi and Yoshimura, Takenori and Wu, Peter and Shi, Jiatong and Saeki, Takaaki and Ju, Yooncheol and Yasuda, Yusuke and Takamichi, Shinnosuke and Watanabe, Shinji},
  journal={arXiv preprint arXiv:2110.07840},
  year={2021}
}
@inproceedings{li2020espnet,
  title={{ESPnet-SE}: End-to-End Speech Enhancement and Separation Toolkit Designed for {ASR} Integration},
  author={Chenda Li and Jing Shi and Wangyou Zhang and Aswin Shanmugam Subramanian and Xuankai Chang and Naoyuki Kamo and Moto Hira and Tomoki Hayashi and Christoph Boeddeker and Zhuo Chen and Shinji Watanabe},
  booktitle={Proceedings of IEEE Spoken Language Technology Workshop (SLT)},
  pages={785--792},
  year={2021},
  organization={IEEE},
}
@inproceedings{arora2021espnet,
  title={{ESPnet-SLU}: Advancing Spoken Language Understanding through ESPnet},
  author={Arora, Siddhant and Dalmia, Siddharth and Denisov, Pavel and Chang, Xuankai and Ueda, Yushi and Peng, Yifan and Zhang, Yuekai and Kumar, Sujay and Ganesan, Karthik and Yan, Brian and others},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7167--7171},
  year={2022},
  organization={IEEE}
}
@inproceedings{shi2022muskits,
  author={Shi, Jiatong and Guo, Shuai and Qian, Tao and Huo, Nan and Hayashi, Tomoki and Wu, Yuning and Xu, Frank and Chang, Xuankai and Li, Huazhe and Wu, Peter and Watanabe, Shinji and Jin, Qin},
  title={{Muskits}: an End-to-End Music Processing Toolkit for Singing Voice Synthesis},
  year={2022},
  booktitle={Proceedings of Interspeech},
  pages={4277-4281},
  url={https://www.isca-speech.org/archive/pdfs/interspeech_2022/shi22d_interspeech.pdf}
}
@inproceedings{lu22c_interspeech,
  author={Yen-Ju Lu and Xuankai Chang and Chenda Li and Wangyou Zhang and Samuele Cornell and Zhaoheng Ni and Yoshiki Masuyama and Brian Yan and Robin Scheibler and Zhong-Qiu Wang and Yu Tsao and Yanmin Qian and Shinji Watanabe},
  title={{ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding}},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={5458--5462},
}
@inproceedings{gao2023euro,
  title={{EURO: ESP}net unsupervised {ASR} open-source toolkit},
  author={Gao, Dongji and Shi, Jiatong and Chuang, Shun-Po and Garcia, Leibny Paola and Lee, Hung-yi and Watanabe, Shinji and Khudanpur, Sanjeev},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}
@inproceedings{peng2023reproducing,
  title={Reproducing {W}hisper-style training using an open-source toolkit and publicly available data},
  author={Peng, Yifan and Tian, Jinchuan and Yan, Brian and Berrebbi, Dan and Chang, Xuankai and Li, Xinjian and Shi, Jiatong and Arora, Siddhant and Chen, William and Sharma, Roshan and others},
  booktitle={2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
  pages={1--8},
  year={2023},
  organization={IEEE}
}
@inproceedings{sharma2023espnet,
  title={ESPnet-{SUMM}: Introducing a novel large dataset, toolkit, and a cross-corpora evaluation of speech summarization systems},
  author={Sharma, Roshan and Chen, William and Kano, Takatomo and Sharma, Ruchira and Arora, Siddhant and Watanabe, Shinji and Ogawa, Atsunori and Delcroix, Marc and Singh, Rita and Raj, Bhiksha},
  booktitle={2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
  pages={1--8},
  year={2023},
  organization={IEEE}
}
@article{jung2024espnet,
  title={{ESPnet-SPK}: full pipeline speaker embedding toolkit with reproducible recipes, self-supervised front-ends, and off-the-shelf models},
  author={Jung, Jee-weon and Zhang, Wangyou and Shi, Jiatong and Aldeneh, Zakaria and Higuchi, Takuya and Theobald, Barry-John and Abdelaziz, Ahmed Hussen and Watanabe, Shinji},
  journal={Proc. Interspeech 2024},
  year={2024}
}
@inproceedings{yan-etal-2023-espnet,
    title = "{ESP}net-{ST}-v2: Multipurpose Spoken Language Translation Toolkit",
    author = "Yan, Brian  and
      Shi, Jiatong  and
      Tang, Yun  and
      Inaguma, Hirofumi  and
      Peng, Yifan  and
      Dalmia, Siddharth  and
      Pol{\'a}k, Peter  and
      Fernandes, Patrick  and
      Berrebbi, Dan  and
      Hayashi, Tomoki  and
      Zhang, Xiaohui  and
      Ni, Zhaoheng  and
      Hira, Moto  and
      Maiti, Soumi  and
      Pino, Juan  and
      Watanabe, Shinji",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    year = "2023",
    publisher = "Association for Computational Linguistics",
    pages = "400--411",
}

@article{ma2022visual,
  title={{Visual Speech Recognition for Multiple Languages in the Wild}},
  author={Ma, Pingchuan and Petridis, Stavros and Pantic, Maja},
  journal={{Nature Machine Intelligence}},
  volume={4},
  pages={930--939},
  year={2022}
  url={https://doi.org/10.1038/s42256-022-00550-z},
  doi={10.1038/s42256-022-00550-z}
}
```